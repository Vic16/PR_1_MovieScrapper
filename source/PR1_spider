## Importamos las librerías necesarias

import scrapy
from scrapy.crawler import CrawlerProcess
import pandas
import json
from scrapy.item import Item, Field
import time
import random

# Creamos la araña.
class uoc_spider(scrapy.Spider):

    # Asignamos un nombre a la araña.
    name = "uoc_spider"

    # Indicamos la url que queremos analizar en primer lugar.
    start_urls = [
        "https://en.wikipedia.org/wiki/List_of_Academy_Award-winning_films"
    ]

    custom_settings = {
        'FEEDS': { 'data_wiki.json': {'format': 'json', 'overwrite': True, 'encofing':'utf-8'}}
        }

    # Definimos el analizador.        
    def parse(self, response):
        
        # Extraemos la info de la película
        for name in response.xpath('//table[@class="wikitable sortable"]//tr'):
            time.sleep(random.uniform(0.20, 0.70))  # Establecemos tiempos de espera aleatorios.
            
            # Capturamos las películas ganadoras
            if name.xpath('.//b').get() is not None:
                best_movie = True
            
            else:
                best_movie = False
            
            yield {
               'title': name.xpath('.//i//text()').get(),
               'year': name.xpath('(.//a)[2]/text()').get(),
               'best movie': best_movie,
               'awards': name.xpath('(./td)[3]/text()').get(),
               'nominations': name.xpath('(./td)[4]/text()').get(),
               'linkMovie': str(name.xpath('.//i//a/@href').get())

            }       
        
        # Iteramos sobre los links
        for item in response.xpath('//table[@class = "wikitable sortable"]//i//a/@href'):
            time.sleep(random.uniform(0.20, 0.70))
            
            if item is not None:
                yield response.follow(item.get(), callback=self.parse_movies)
                
            
     # Capturamos la información de cada película
    def parse_movies(self, response):
        
        response_type = ['"Directed"', '"Screenplay"', 
                         '"Starring"', '"Produced"', 
                         "'Cinematography'", "'Edited'",
                         "'Music'", "'Production company'",
                         "'Distributed'", "'Release dates'",
                         "'Running time'", "'Country'",
                         "'Language'", "'Budget'",
                         "'Box office'"
                        ]
        
        for tipo in response_type:
            
            path_template = '//table[@class="infobox vevent"]//th[contains(text(), {})]/following-sibling::td'.format(tipo)
            
            if response.xpath(path_template+'//ul').get() is not None:
                path_template = path_template+'//li'
            
            for name in response.xpath(path_template):
                yield {'movie': response.xpath('//h1[@id="firstHeading"]/i/text()').get(),
                       tipo: name.xpath('.//text()').get(),
                       'link': response.request.url,
                    }



        # Links de los actores
        path_template = '//table[@class="infobox vevent"]//th[contains(text(), "Starring")]/following-sibling::td'
        
        if response.xpath(path_template+'//ul').get() is not None:
            path_template = path_template+'//li'
        
        for item in response.xpath(path_template+'//@href'):
            
            if item is not None:
                yield response.follow(item.get(), callback=self.parse_cast)
                time.sleep(random.uniform(0.20, 0.70))
                
            
     # Capturamos la información de cada persona
    def parse_cast(self, response):
        
        time.sleep(random.uniform(0.20, 0.70))
        
        # Loop para determinar el género de la persona
        females_sum = 0
        males_sum = 0
        female_list = ['" her "', '"Her "', 
                       '" she "', '"She "', 
                       '" female "', 
                       '" actress "', 
                       '" woman "']
        male_list = ['" his "', '"His "', 
                     '" he "', '"He "', 
                     '" male "', 
                     '" man "']

        
        if response.xpath('//div[@class="catlinks"]//a[contains(text(), "women") or contains(text(), "actresses") or contains(text(), "females")]').get() is not None:
            female = True
        elif response.xpath('//div[@class="catlinks"]//a[contains(text(), "men") or contains(text(), "actors") or contains(text(), "males")]').get() is not None:
            female = False 
        else:
            for e in response.xpath('//p'):
                for word_fem in female_list:
                    path = 'count(.//text()[contains(.,{})])'.format(word_fem)
                    females_sum = females_sum + float(e.xpath(path).get())
                    
                for word_male in male_list:
                    path = 'count(.//text()[contains(., {})])'.format(word_male)
                    males_sum = males_sum + float(e.xpath(path).get())

            if females_sum > males_sum:
                female = True
            elif females_sum < males_sum:
                female = False
            else:
                female = 'NA'

            
        yield {'name': response.xpath('//table[@class="infobox biography vcard"]//tr//div[@class="fn"]/text()').get(),
               'female': female,
              'birthdate': response.xpath('//table[@class="infobox biography vcard"]//tr//span[@class="bday"]/text()').get(),
               #Birthplace sale regular
               'birthplace': response.xpath('//table[@class="infobox biography vcard"]//tr//div[@class="birthplace"]//text()').getall(),
               'link': response.request.url
              }
                     
            
if __name__ == "__main__":
# Creamos un crawler
  process = CrawlerProcess({'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',
                    'DOWNLOAD_HANDLERS': {'s3': None},
                    'LOG_ENABLED': True
                })

# Inicializamos el crawler con nuestra araña.
process.crawl(uoc_spider)

                # Lanzamos la araña
process.start()